# -*- coding: utf-8 -*-
"""InnovationHackathonYtVideo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xmhdZFmPUFDx4jSDKpEU8dlpW92wzl_E

# **Install the required modules**
"""

!pip install youtube-transcript-api
!pip install openai
!pip install pytube
!pip install openai-whisper
!pip install google-generativeai
!pip install yt-dlp

"""# **Import the modules**"""

import google.generativeai as genai
from openai import OpenAI
import os
from pytube import YouTube
from youtube_transcript_api import YouTubeTranscriptApi

"""# **Configuring the Gemini API**"""

# Configure i.e loading the API Key for communication.
genai.configure(api_key="API_KEY")

"""# **Configuring the OpenAI API**"""

import openai
openai.api_key = "API_KEY"
client = openai.OpenAI(api_key=openai.api_key) # Pass the API key to the constructor

"""# **Conversion of Video into Audio file for Text conversion using OpenAI**"""

import os
import subprocess

def download_and_rename_audio(url, new_name="audio1"):
    # Step 1: Download the video and extract audio
    command = ['yt-dlp', '--extract-audio', '--audio-format', 'mp3', '--output', f'{new_name}.%(ext)s', url]
    subprocess.run(command, check=True)

    # Step 2: Find the downloaded audio file
    filenames = os.listdir('.')
    old_path = None
    for filename in filenames:
        if filename.startswith(new_name) and filename.endswith('.mp3'):
            old_path = filename
            break

    if old_path is None:
        print("Error: No downloaded audio file found.")
        return None

    # Step 3: Store the new path
    new_file_path = os.path.join('.', old_path)
    print(f"New file path: {new_file_path}")
    return new_file_path

"""# **Getting the Transcript of the video provided**"""

!pip install yt-dlp

def get_transcript_text(video_id):
    try:
        # Retrieve the transcript
        print("In try")
        transcript = YouTubeTranscriptApi.get_transcript('en', video_id)
        print("In try")

        # Extract and print only the text
        transcript_text = ' '.join([entry['text'] for entry in transcript])
        return transcript_text
    except Exception as e:
        # print(f"Error: {e}")
        print("Error Occured while getting transcript")
        print("Using OpenAI to convert the YouTube video into text")
        video_url = f"https://www.youtube.com/watch?v={video_id}"

        # Download the video
        download_and_rename_audio(video_url)
        # print("The Path of downloaded file is /content/vid1.mp4")

"""# **Audio To Text using OpenAI**"""

get_transcript_text("_Ki4bS4V2gQ")

# !pip install git+https://github.com/openai/whisper.git
# !apt update && apt install -y ffmpeg
# !apt update && apt install -y ffmpeg

import whisper
import time

# Load the smallest Whisper model for faster processing
model = whisper.load_model("tiny")

# Path to the audio file
audio_file_path = "/content/audio1.mp3"

# Start timing
start_time = time.time()

# Transcribe the audio file
result = model.transcribe(audio_file_path)

# End timing
end_time = time.time()

# Print the transcription and processing time
print("Transcription:", result["text"])
print("Processing time:", end_time - start_time, "seconds")

result

result["text"]

"""# **Using Gemini to get the Transcript**"""

# import time

# # Initialize a Gemini model appropriate for your use case.
# model = genai.GenerativeModel(model_name="gemini-1.5-pro")

# # Create the prompt.
# prompt = "Generate a transcript of the speech."

# # Path to the audio file
# audio_file = genai.upload_file(path='/content/audio1.mp3')

# # Start timing
# start_time = time.time()

# # Pass the prompt and the audio file to Gemini.
# response = model.generate_content([prompt, audio_file])

# # End timing
# end_time = time.time()

# # Print the transcript.
# print(response.text)
# print("Processing time:", end_time - start_time, "seconds")
import time

# Initialize a Gemini model appropriate for your use case.
model = genai.GenerativeModel(model_name="gemini-1.5-pro")

# Create the prompt.
prompt = "Generate a transcript of the speech."

# Path to the audio file
audio_file_path = '/content/audio1.mp3'

# Check if the file exists
import os
if not os.path.exists(audio_file_path):
    raise FileNotFoundError(f"File not found: {audio_file_path}")

# Upload the audio file
audio_file = genai.upload_file(path=audio_file_path)

# Start timing
start_time = time.time()

# Pass the prompt and the audio file to Gemini.
try:
    response = model.generate_content([prompt, audio_file])

    # End timing
    end_time = time.time()

    # Print the transcript.
    print(response.text)
    print("Processing time:", end_time - start_time, "seconds")

except Exception as e:
    print(f"An error occurred: {e}")

"""# **Storing the transcript into KB**"""

knowledge_base = result["text"]
knowledge_base

"""# **Using Gemini 1.5 flash for response generatinon**"""

# Taking question as input
question = input("Enter your question: ")

# Initialize a Gemini model appropriate for your use case.
model = genai.GenerativeModel(model_name="gemini-1.5-flash")

# Defining promt for the required response
promt = """You are an expert in finding answers or matching answers for the asked question from the Knowledge Base Given below.
           Your task is to analyze the complete Knowledge Base and answer the questions asked.
           The Knowledge Base is : """ + knowledge_base + """The Question is""" + question + """ The Output Must be like Question:<Asked Question> And in new Line Answer: <Answer Generated>"""

# Pass the prompt and the audio file to Gemini.
response = model.generate_content([promt])

# Print the transcript.
print(response.text)

"""# **Generating Multiple Questions for the KB**"""

n_questions = input("Enter the number of questions you want to generate: ")

# Defining promt for the required response
questions_promt = """You are an expert in framing the number of questions asked.
           Your task is to analyze the complete Knowledge Base and generate the number of questions asked.
           The Knowledge Base is : """ + knowledge_base + """The Number of Questions need to be Generated is""" + n_questions + """The output must be 1.<Question1> 2.<Question2> 3.<Question3> upto specified number of questions And provide answer for the question."""

# Pass the prompt and the audio file to Gemini.
question_response = model.generate_content([questions_promt])

# Print the transcript.
print(question_response.text)

"""# **Summarizing the transcript of the YouTube video**"""

# Specifing the Summarizing prompt
summary_promt = """You are an expert in English Language
                   Now your task is to summarize the given content into 250 words and remove any gramatical mistakes.
                   The summary is :-""" + knowledge_base + """ Generate the Important points in each line"""

# Pass the prompt and the audio file to Gemini.
summary_response = model.generate_content([summary_promt])

# Print the transcript.
print(summary_response.text)

"""# **Generating the MCQs questons for it.**"""

mcqs_prompt = """You are an expert in framing the number of MCQ questions asked.
           Your task is to analyze the complete Knowledge Base and generate the number of questions asked.
           The Knowledge Base is : """ + knowledge_base + """The Number of Questions need to be Generated is""" + n_questions + """The output must be 1.<Question1> a. opt1 b. op2 c. opt3 d. opt4  2.<Question2>a. opt1 b. op2 c. opt3 d. opt4 3.<Question3>a. opt1 b. op2 c. opt3 d. opt4 upto specified number of questions"""

# Pass the prompt and the audio file to Gemini.
mcqs_response = model.generate_content([mcqs_prompt])

# Print the transcript.
print(mcqs_response.text)

"""# **Export into PDF**"""



"""# **Test Purpose Not fully Implemented from here**

# **RAG Implementation for above approach along with Gemini API**
"""

!pip install sentence_transformers

import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Configure the Gemini API
genai.configure(api_key="AIzaSyAIewGMqAtMEtZMZjDJgEPNEwh_Q74yfGw")

# Initialize the Gemini model
model = genai.GenerativeModel(model_name="gemini-1.5-flash")

# Initialize the sentence transformer model for embedding
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')

# Split the knowledge base into chunks
def split_into_chunks(text, chunk_size=100):
    words = text.split()
    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

chunks = split_into_chunks(knowledge_base)

# Create embeddings for all chunks
chunk_embeddings = sentence_model.encode(chunks)

def get_most_relevant_chunk(query, top_k=3):
    query_embedding = sentence_model.encode([query])
    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return [chunks[i] for i in top_indices]

def rag_query(query):
    relevant_chunks = get_most_relevant_chunk(query)
    context = "\n".join(relevant_chunks)

    prompt = f"""You are an AI assistant trained to answer questions based on the given context.
    Use the following context to answer the question. If the answer is not in the context, say "I don't have enough information to answer this question."

    Context: {context}

    Question: {query}

    Answer:"""

    response = model.generate_content([prompt])
    return context, response.text

# Example usage
while True:
    question = input("Enter your question (or 'quit' to exit): ")
    if question.lower() == 'quit':
        break
    context, answer = rag_query(question)
    print(f"Question: {question}")
    print(f"Context:\n{context}\n")
    print(f"Answer: {answer}\n")
    print("-" * 50)  # Separator for readability

# chunk_embeddings

# chunks

"""# **Implementation of RAG for above using FAISS and cosine similarity**"""

!pip install faiss-cpu

# Import the required modules
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# Create a FAISS index
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(chunk_embeddings.astype('float32'))

print(f"Dimension: {dimension}")
print(f"Index: {index}")

# Store the chunks and their IDs
chunk_dict = {i: chunk for i, chunk in enumerate(chunks)}

# chunk_dict

# Save the index and chunk dictionary
faiss.write_index(index, "transcript_index.faiss")
with open("chunk_dict.pkl", "wb") as f:
    pickle.dump(chunk_dict, f)

def load_vector_db():
    # Load the index and chunk dictionary
    index = faiss.read_index("transcript_index.faiss")
    with open("chunk_dict.pkl", "rb") as f:
        chunk_dict = pickle.load(f)
    return index, chunk_dict

def get_most_relevant_chunk(query, index, chunk_dict, top_k=3):
    query_embedding = sentence_model.encode([query]).astype('float32')
    distances, indices = index.search(query_embedding, top_k)
    return [chunk_dict[int(i)] for i in indices[0]]

def rag_query(query, index, chunk_dict):
    relevant_chunks = get_most_relevant_chunk(query, index, chunk_dict)
    context = "\n".join(relevant_chunks)

    prompt = f"""You are an AI assistant trained to answer questions based on the given context.
    Use the following context to answer the question. If the answer is not in the context, say "I don't have enough information to answer this question."

    Context: {context}

    Question: {query}

    Answer:"""

    response = model.generate_content([prompt])
    return context, response.text

# Load the vector database
index, chunk_dict = load_vector_db()

print(f"Index: {index}")
print(f"Chunk Dictionary: {chunk_dict}")

# Example usage
while True:
    question = input("Enter your question (or 'quit' to exit): ")
    if question.lower() == 'quit':
        break
    context, answer = rag_query(question, index, chunk_dict)
    print(f"Question: {question}")
    print(f"Context:\n{context}\n")
    print(f"Answer: {answer}\n")
    print("-" * 50)  # Separator for readability

"""# **Implementing only RAG without Gemini API to test the accuracy in the response**"""

# Import the required modules
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Preprocess the knowledge base
def preprocess(text):
    sentences = sent_tokenize(text)
    stop_words = set(stopwords.words('english'))
    processed_sentences = []
    for sentence in sentences:
        words = word_tokenize(sentence.lower())
        words = [word for word in words if word.isalnum() and word not in stop_words]
        processed_sentences.append(' '.join(words))
    return processed_sentences

processed_kb = preprocess(knowledge_base)
processed_kb

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(processed_kb)

# tfidf_matrix

def get_most_relevant_sentence(query):
    query_vector = vectorizer.transform([query])
    similarities = cosine_similarity(query_vector, tfidf_matrix)
    most_similar_idx = np.argmax(similarities)
    return sent_tokenize(knowledge_base)[most_similar_idx]

def answer_question(question):
    relevant_sentence = get_most_relevant_sentence(question)
    return f"Based on the information provided: {relevant_sentence}"

# Main loop
while True:
    user_question = input("Ask a question about the Nothing CMF phone (or type 'exit' to quit): ")
    if user_question.lower() == 'exit':
        break
    answer = answer_question(user_question)
    print(answer)
    print()

"""# **Based on the above understanding, we must use both the RAG Application along with the LLM to provide much more accurate results. If we use only the RAG, we can't get the optimal results. By using RAG along with the Gemini API, we provide extra context that the answer mostly lies in the context for which the searching for the appropriate query time taken will be very less.**"""

